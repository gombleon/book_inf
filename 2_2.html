<!DOCTYPE html>
<html>
<meta charset="UTF-8" />
<link rel='stylesheet' type="text/css" href="style.css"></link>
<meta name="author" content="dima" />
<meta name="description" content="энтропия по Шеннону" />
<meta name="keywords" content="формула Шеннона, энтропия, неравновероятные исходы" />
<head>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML"],
  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<title>Формула Шеннона</title>

</head>
<body>
<p><a href="2_1.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="2_3.html">К следующему</a></p>
<h2>2.2. Вероятностный подход Шеннона к вычислению энтропии</h2>
<p>Главный недостаток формулы Хартли заключается в требовании равновероятности исходов случайного события.
Поэтому для вычисления энтропии в случайном событии с неравновероятными исходами использовать формулу Хартли нельзя.</p>
<p>Для случая неравновероятных исходов нужна другая формула, и ее предложил К. Шеннон в своей работе <a href="litra.html#41">&quot;Математическая теория связи&quot; [42]</a>.</p>
<p>Очевидно, что <a href="2_1.html#f2_1_3"> формулу Хартли (2.1.3)</a> можно представить так:
<table  class="formula">
<tr>
<td class="formula">
<math display="block">
	<mi>H</mi>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mi>n</mi>
	</mrow>
	<mo>=</mo>
	<mrow>
		<mo>-</mo><mi>n</mi>
		<mo>&#x2061;</mo>
		<mfrac><mn>1</mn><mi>n</mi></mfrac>
		<mo>&#x2061;</mo>
		<mrow>
			<msub><mi>log</mi><mn>2</mn></msub>
			<mo>&#x2061;</mo>
			<mfrac><mn>1</mn><mi>n</mi></mfrac>
		</mrow>
	</mrow>
	<mo>.</mo>
</math>
</td>
<td class="nomer">(2.2.1)</td>
</tr>
</table>
</p>
<p>Тогда неопределенность, вносимую каждым из равновероятных исходов, можно истолковывать как величину 
<math display='inline'>
<mrow>
		<mrow><mo>-</mo><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow>
	<mo>&#x2062;</mo>
	<mrow>	
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mfrac><mn>1</mn><mi>n</mi></mfrac>
	</mrow>
</mrow>
</math>,
 неопределенность в случайном событии &#8212; как сумму <var>n</var> таких слагаемых, величину
<math display='inline'>
	<mfrac><mn>1</mn><mi>n</mi></mfrac>
</math> &#8212; как вероятность исхода.</p>
<p>Пусть случайное событие имеет <var>n</var> исходов с <a href="pril.html">вероятностями</a>
<math display='inline'>
	<msub><mi>p</mi><mn>1</mn></msub>
	<mo>,</mo>
	<mi>&#8230;</mi>
	<mo>,</mo>
	<msub><mi>p</mi><mi>n</mi></msub>
</math>. Величину 
<math display='inline'>
	<mrow>
		<mo>-</mo><msub><mi>p</mi><mi>i</mi></msub>
	</mrow>	
	<mo>&#x2062;</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<msub><mi>p</mi><mi>i</mi></msub>
	</mrow>
</math>
можно истолковывать как неопределенность, вносимую <var>i</var>-м исходом случайного события, <var>i=1,..., n</var>.
</p>
<p><a name="entropiy"></a>Для вычисления средней неопределенности в случайном событии, очевидно, надо составить сумму:
<a name="f2_2_2"></a>
<table class="formula"><tr>
<td class="formula">
<math   display="block">
	<mi>H</mi>
	<mo>=</mo>
	<mrow>	
		<mo>-</mo>
		<munderover>
   		<mo>&#931;</mo>
   			<mrow>
        			<mi>i</mi>
        			<mo>=</mo>
        			<mn>1</mn>
   			</mrow>
   			<mi>n</mi>
		</munderover>
		<msub><mi>p</mi><mi>i</mi></msub>
		<mo>&#x2062;</mo>
		<mrow>
			<msub><mi>log</mi><mn>2</mn></msub>
			<mo>&#x2061;</mo>
			<msub><mi>p</mi><mi>i</mi></msub>
		</mrow>
	</mrow>
<mo>.</mo>
</math>
</td>
<td class="nomer">(2.2.2)</td>
</tr></table>
</p>
<p>Фон Нейман посоветовал К. Шеннону назвать эту величину энтропией, так как формула для ее вычисления совпала с соответствующей формулой статистической физики.</p>
<p><span class="teorema">Энтропия - мера неопределенности случайного события или мера нашего незнания исхода случайного события. Сообщение о случайном событии раскрывает эту неопределенность и несет количество информации, равное энтропии.</span></p>
<a name="inf_emk"></a>
<p><span class="teorema">Под информационной емкостью сообщения будем понимать произведение энтропии сообщения на количество символов в сообщении.</span></p>
<div class="examp">
<p><span class="primer">Пример 2.2.1</span>. Вычислить информационную емкость текста: &quot;теорема о среднем&quot;.</p>
<p><span class="primer">Решение</span>. Составим статистическую таблицу:
<table border="1">
<tr>
<td>Алфавит</td>
<td>т</td>
<td>а</td>
<td>д</td>
<td>е</td>
<td>м</td>
<td>н</td>
<td>о</td>
<td>р</td>
<td>с</td>
<td>"пробел"</td>
</tr>
<tr>
<td>Частота</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Относительная частота</td>
<td>0.06</td>
<td>0.06</td>
<td>0.06</td>
<td>0.24</td>
<td>0.12</td>
<td>0.06</td>
<td>0.12</td>
<td>0.12</td>
<td>0.06</td>
<td>0.12</td>
</tr>
</table>
</p>
<p>Чаще всего (4 раза) в тексте встречается буква &quot;е&quot;. Она наиболее вероятная буква в рассматриваемом сообщении. Поэтому с этой буквой соотносится наименьшая неопределенность, равная 
<math display='inline'>
<mrow>
	<mrow><mo>-</mo><mn>0.24</mn></mrow>
	<mo>&#x2062;</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mn>0.24</mn>
	</mrow>
</mrow>
</math>.
 Наибольшая неопределенность, равная
<math display='inline'>
<mrow>
	<mrow><mo>-</mo><mn>0.06</mn></mrow>
	<mo>&#x2062;</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mn>0.06</mn>
	</mrow>
</mrow>
</math>,
 за редкими символами &quot;т&quot;, &quot;а&quot;, &quot;д&quot;, &quot;н&quot;, &quot;с&quot;.</p>
<p>Применение формулы Шеннона (2.2.2) дает энтропию, равную 3.15 бит. Такова средняя неопределенность, связанная с появлением очередного символа. Сообщение (подсказка) об очередном символе будет содержать среднее количество информации, равное энтропии в 3.15 бит. Для нахождения информационной емкости всего текста умножим количество символов 17 в тексте на энтропию 3.15 бит и получим 53.55 бит.</p>
</div>
<p>Если энтропия 
<math display='inline'>
	<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math>
является мерой неопределенности случайного события <var>&#945;</var>, то количество информации 
<math display='inline'>
	<mi>I</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math>
 характеризует уменьшение или снятие неопределенности в результате появления случайного события. Энтропия
<math display='inline'>
	<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math> 
 выражает среднюю неопределенность случайного события <var>&#945;</var>, является ее априорной характеристикой и может быть вычислена до эксперимента, если известна статистика состояний. Количество же информации
<math display='inline'>
	<mi>I</mi><mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math> 
 определяется апостериорно, т.е. в результате эксперимента. Числовое совпадение
<math display='inline'>
	<mi>H</mi><mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math> 
 и 
<math display='inline'>
	<mi>I</mi><mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math> 
 возможно лишь в том случае, если эксперимент полностью снимает априорную неопределенность.</p>
<p>Равенство нулю 
<math display='inline'>
	<mi>H</mi><mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</math>
 означает, что исход случайного события (или опыта) <var>&#945;</var> заранее известен. Большее или меньшее значение энтропии означает большую или меньшую неопределенность исхода случайного события (или результата опыта) <var>&#945;</var>. Допустим, что опыт (или исход случайного события) <var>&#945;</var> предшествует опыту (или исходу случайного события) <var>&#945;</var>. В исходе события <var> &#946;</var> может быть некоторая информация об исходе <var>&#945;</var>, что означает уменьшение энтропии <var>&#945;</var> в результате реализации опыта <var> &#946;</var>. Мы можем трактовать <var> &#946;</var> как вспомогательный опыт для снятия части неопределенности опыта <var>&#945;</var>. Обозначим как
<math display='inline'>
<mrow>
	<msub><mi>H</mi><mi>&#946;</mi></msub>
	<mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>&#945;</mi><mo>)</mo></mrow>
</mrow>
</math>
 остаточную энтропию случайного события <var>&#945;</var> после реализации <var> &#946;</var>. Тогда количество информации 
<math  >
	<mi>I</mi><mo>&#x2061;</mo><mrow><mfenced><mi>&#946;</mi><mi>&#945;</mi></mfenced></mrow>
</math> 
 о случайном событии <var>&#945;</var>, содержащаяся в случайном событии <var> &#946;</var>, выразится как уменьшение неопределенности исхода случайного события <var>&#945;</var>:
<table class="formula"><tr>
<td class="formula">
<math display="block">
	<mi>I</mi>
	<mo>&#x2061;</mo>
	<mrow>
		<mfenced><mi>&#946;</mi><mi>&#945;</mi></mfenced>
	</mrow>
	<mo>=</mo>
	<mrow>	
		<mrow>
			<mi>H</mi>
			<mo>&#x2061;</mo>
			<mrow>
				<mo>(</mo><mi>&#945;</mi><mo>)</mo>
			</mrow>
		</mrow>
		<mo>-</mo>
		<mrow>
			<msub><mi>H</mi><mi>&#946;</mi></msub>
			<mo>&#x2061;</mo>
			<mfenced><mi>&#946;</mi></mfenced>
		</mrow>
	<mrow>
<mo>.</mo>
</math></td>
<td class="nomer">(2.2.3)</td>
</tr></table></p>
<p>Возможно, что исход опыта <var> &#946;</var> полностью снимет неопределенность исхода опыта <var>&#945;</var>, и тогда остаточная энтропия <var>H<sub> &#946;</sub>(&#945;)</var> окажется равной нулю, а
<table class="formula"><tr>
<td class="formula">
<math display="block">
	<mrow>
		<mi>I</mi>
		<mo>&#x2061;</mo>
		<mfenced><mi>&#946;</mi><mi>&#945;</mi></mfenced>
	</mrow>
	<mo>=</mo>
		<mrow>
			<mi>H</mi>
			<mo>&#x2061;</mo>
			<mrow>
				<mo>(</mo><mi>&#945;</mi><mo>)</mo>
			</mrow>
		</mrow>
<mo>.</mo>
</math></td>
<td class="nomer">(2.2.4)</td>
</tr></table>
</p>
<p>Если случайные события <var>&#945;</var> и <var> &#946;</var> независимые, то исход случайного события <var> &#946;</var> не уменьшает энтропию случайного события <var>&#945;</var>:
<table class="formula"><tr>
<td class="formula">
<math display="block">
	<mrow>
		<mi>H</mi>
		<mo>&#x2061;</mo>
		<mrow>
			<mo>(</mo><mi>&#945;</mi><mo>)</mo>
		</mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>H</mi><mi>&#946;</mi></msub>
		<mo>&#x2061;</mo>
		<mfenced><mi>&#946;</mi></mfenced>
	</mrow>
	<mo>.</mo>
</math>
</td>
<td class="nomer">(2.2.5)</td>
</tr></table>
</p>
<p>Определение количества информации через понятие энтропии называют еще <span class="termin">синтаксической</span> мерой информации, кроме которой различают еще <span class="termin">семантическую</span> и <span class="termin">прагматическую</span> меры информации.</p>
<p>Семантическая или тезаурусная мера информации является относительной величиной и служит для измерения смыслового содержания информации. Она связывает семантические свойства информации со способностью пользователя воспринимать сообщение.</p>
<p><span class="teorema">Тезаурус - это совокупность сведений, которыми располагает пользователь или система.</span></p>
<p>Количество смысловой информации 
<math display='inline'>
	<msub><mi>I</mi><mi>p</mi></msub>
</math>, воспринимаемой пользователем, зависит от смыслового содержания <var>S</var> сообщения и тезауруса пользователя
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
</math>:		
<math display='inline'>		
<mrow>
	<msub><mi>I</mi><mi>p</mi></msub>
	<mo>=</mo>
	<mrow>
		<mi>f</mi>
		<mo>&#x2061;</mo>
		<mfenced><mi>S</mi><msub><mi>S</mi><mi>p</mi></msub></mfenced>
	</mrow>
</mrow>
</math>.
Неотрицательная функция 
<math display='inline'>
<mrow>
	<msub><mi>I</mi><mi>p</mi></msub>
	<mo>=</mo>
	<mrow>
		<mi>f</mi>
		<mo>&#x2061;</mo>
		<mfenced><mi>S</mi><msub><mi>S</mi><mi>p</mi></msub></mfenced>
	</mrow>
</mrow>
</math>
 равна нулю при 
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
	<mo>=</mo>
	<mn>0</mn>
</math> 
 и 
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
	<mo>&#8805;</mo>
	<msub><mi>S</mi><mi>max</mi></msub>
</math>. В случае равенства нулю своего тезауруса 
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
</math>
пользователь не может воспринять смысловое содержание сообщения, а в случае 
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
	<mo>&#8805;</mo>
	<msub><mi>S</mi><mi>max</mi></msub>
</math>
сообщение не cодержит сведений, отсутствующих в его тезаурусе. Во всех остальных случаях пользователь осмысливает сообщение и пополняет свой тезаурус новыми сведениями. Функция 
<math display='inline'>
<mrow>
		<msub><mi>I</mi><mi>p</mi></msub>
	<mo>=</mo>
	<mrow>
		<mi>f</mi>
		<mo>&#x2061;</mo>
		<mfenced><mi>S</mi><msub><mi>S</mi><mi>p</mi></msub></mfenced>
	</mrow>
</mrow>
</math>
по переменной 
<math display='inline'>
	<msub><mi>S</mi><mi>p</mi></msub>
</math>
возрастает на интервале от 0 до некоторого значения 
<math display='inline'>
	<msub><mi>S</mi><mi>opt</mi></msub>
</math>
и убывает на интервале от 
<math  >
<msub><mi>S</mi><mi>opt</mi></msub>
</math>
до
<math display='inline'>
	<msub><mi>S</mi><mi>max</mi></msub>
</math>.</p>
<p>Прагматическая мера информации определяет полезность информации  для достижения пользователем поставленной цели. Эта мера информации тоже является относительной величиной, обусловленной использованием информации в той или иной системе.</p>
<h3>Контрольные вопросы и упражнения</h3>
<ol>
<li>Вероятность попадания в цель первым стрелком равна 0.3, а вторым &#8212; 0.7. С выстрелом какого стрелка связана большая неопределенность?</li>
<li>Вопрос имеет два варианта ответа. Возможно ли, чтобы с каждым из ответов было связано различное количество информации?</li>
<li>Какое количество информации содержит каждый из ответов на вопрос, если всего их 3 и все они равновероятны? А если равновероятных ответов  n?</li> 
<li>Источник порождает множество шестизнаковых сообщений, каждое из которых содержит 1 знак &quot;*&quot;, 2 знака &quot;%&quot; и 3 знака &quot;!&quot;. Вычислите энтропию сообщения.</li>
<li>Пусть событие  &#946; состоит в случайном извлечении одного шара без возвращения из урны с 9 белыми, 5 красными и 7 черными шарами, а событие &#945; - в повторном извлечении из урны еще одного шара. Вычислите остаточную энтропию <var>H<sub> &#946;</sub>(&#945;)</var> события &#945; после реализации  &#946; и количество информации о случайном событии &#945;, содержащуюся в событии  &#946;.</li>
<li>Пусть событие  &#946; состоит в случайном извлечении из урны с 9 белыми, 5 красными и 7 черными шарами одного шара с последующим его возвращением в урну , а событие &#945; - в повторном извлечении из урны еще одного шара. Вычислите остаточную энтропию <var>H<sub> &#946;</sub>(&#945;)</var> события &#945; после реализации  &#946; и количество информации о случайном событии &#945;, содержащуюся в событии  &#946;.</li>
</ol>
<p><a href="2_1.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="2_3.html">К следующему</a></p>
</body>
<script type="text/javascript">
  MathJax.Hub.Configured()
</script>
</html>