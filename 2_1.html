<!DOCTYPE html>
<html>
<meta charset="UTF-8" />
<link rel='stylesheet' type="text/css" href="style.css"></link>
<meta name="author" content="dima" />
<meta name="description" content="энтропия по Хартли" />
<meta name="keywords" content="формула Хартли, энтропия, количество исходов" />
<head>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML"],
  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<title>Формула Хартли</title></head>
<body>
<p><a href="1_2.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="2_2.html">К следующему</a></p>
<h1>2. Основы математической теории информации</h1>
<p>Применение математических средств к измерению информации возможно в том случае, если отвлечься от содержания информации. Именно такой подход применили Р. Хартли, К. Шеннон, А.Н. Колмогоров, так как &quot;чистая математика&quot; оперирует с количественными соотношениями, не вдаваясь в физическую природу тех объектов, за которыми стоят соотношения. Поэтому, если смысл удален из сообщений, то отправной точкой для информационной оценки события остается только множество отличных друг от друга событий и, соответственно, сообщений о них.</p>
<h2>2.1. Комбинаторный подход Хартли к вычислению энтропии</h2>
<p>В 1928 году Р. Хартли <a href="litra.html#36">[37]</a>, занимавший должность инженера в телеграфной компании, связал количество информации с количеством различных состояний физической системы. Пусть какое-либо <a href="pril.html">случайное событие имеет <var>n</var> равновероятных исходов</a>. Задача состоит в построении <span class="termin">меры неопределенности (энтропии)</span> в исходе случайного события как меры количества информации в сообщении об исходе случайного события. <a href="2_2.html#entropiy">Определение энтропии</a> для случая, когда исходы случайного события могут быть и неравновероятными, будет рассмотрено в следующем параграфе. Обозначим энтропию как <var>H</var>. При построении энтропии Хартли руководствовался следующими требованиями:</p>
<ol>
<li>энтропия должна быть неотрицательной функцией от количества исходов <var>n</var>;</li>
<li>если событие имеет всего один исход,то в этом случае никакой неопределенности нет. Следовательно,
энтропия опыта с единственным исходом должна быть равна нулю: 
<math>
<mrow>
<mi>H</mi>
<mo>&#x2061;</mo>
<mrow>
<mo>(</mo>
<mn>1</mn>
<mo>)</mo>
</mrow>
<mo>=</mo><mn>0</mn>
</mrow>
</math>
;</li>
<li>очевидно, что чем больше количество исходов n, тем больше неопределенности в исходе опыта, и тем больше информации в сообщении об исходе опыта. Поэтому энтропия должна быть возрастающей функцией от количества исходов <var>n</var>;</li>
<li>пусть 
<math><msub><mi>A</mi><mn>1</mn></msub></math>
 и <math><msub><mi>A</mi><mn>2</mn></msub></math> &#8212; два <a href="pril.html#nezs_s">независимых случайных события</a>, первое из которых имеет <math><msub><mi>n</mi><mn>1</mn></msub></math>, а второе &#8212; <math><msub><mi>n</mi><mn>2</mn></msub></math> равновероятных исходов. Случайное событие <var>В</var>, состоящее в совместном появлении событий <math><msub><mi>A</mi><mn>1</mn></msub></math>
 и <math><msub><mi>A</mi><mn>2</mn></msub></math>, будет иметь 
<math><msub><mi>n</mi><mn>1</mn></msub><mo>&#x2062;</mo><msub><mi>n</mi><mn>2</mn></msub></math>
 равновероятных исходов. Энтропия случайного события <var>В</var> должна быть равна сумме энтропий случайных событий <math><msub><mi>A</mi><mn>1</mn></msub></math>
 и <math><msub><mi>A</mi><mn>2</mn></msub></math>:
<table class="formula">
<tr>
<td class="formula">
<math display="block">
<mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mrow><mo>=</mo>
<mrow><mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>)</mo></mrow></mrow> <mo>+</mo><mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow> </mrow><mo>.</mo>
</math>
</td>
<td class="nomer">(2.1.1)</td>
</tr>
</table>
</li>
</ol>	
<p>Перечисленным требованиям удовлетворяет логарифмическая функция по некоторому основанию 
<math><mi>a</mi><mo>&gt;</mo><mn>1</mn></math>:</p>
<table class="formula"><tr>
<td class="formula">
<math display="block">
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo>
			<mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow>
	</mrow>
<mo>=</mo>
<mrow>
	<msub><mi>log</mi><mi>a</mi></msub>
	<mo>&#x2061;</mo><mi>n</mi>
</mrow>
<mo>.</mo>
</math></td>
<td class="nomer">(2.1.2)</td>
</tr>
</table>
<p>Выбор основания логарифма значения не имеет, так как переход к другому основанию 
<math><mi>b</mi></math>
означает изменение масштаба измерения количества информации на величину 
<math>
	<msub><mi>log</mi><mi>b</mi></msub>
	<mo>&#x2061;</mo>
	<mi>a</mi>
</math>:

<math display="block">
<mrow>
	<mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>b</mi></msub>
		<mo>&#x2061;</mo><mi>n</mi>
	</mrow>
	<mo>=</mo>
	<mrow>
		<mrow>
			<msub><mi>log</mi><mi>b</mi></msub>
			<mo>&#x2061;</mo>
			<mi>a</mi>
		</mrow>
		<mo>&#x2062;</mo>
		<mrow>
			<msub><mi>log</mi><mi>a</mi></msub>
			<mo>&#x2061;</mo>
			<mi>n</mi>
		</mrow>
	</mrow>
	<mo>.</mo>
</mrow>
</math>
</p>
<p>Если в качестве основания логарифма взять 2, то мерой информации будет служить неопределенность, содержащаяся в опыте с двумя равновероятными исходами (<var>TRUE</var> и <var>FALSE</var>), и можно использовать для анализа таких событий аппарат математической логики.</p>
<p><span class="defin"> Единица измерения неопределенности при двух возможных равновероятных исходах опыта называется бит</span>.</p>

<p>Название происходит от английского binary digit, что в дословном переводе означает &quot;двоичный разряд&quot; или &quot;двоичная единица&quot;.</p>
<p>В случае основания <var>e</var> единица информации называется <span class="termin">натом</span>, а для основания логарифма 10  &#8212; <span class="termin">Хартли</span> в честь Р. Хартли, который первым предложил логарифмическую меру измерения информации.</p>
<p><a name="f2_1_3"></a>Таким образом, явный вид функции для описания меры неопределенности или энтропии опыта, имеющего <var>n</var> равновероятных исходов, следующий (<span class="termin">формула Хартли</span>):
<table class="formula">
<tr>
<td class="formula">
<math display="block">
	<mrow>
		<mi>H</mi><mo>(</mo><mi>n</mi><mo>)</mo>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mi>n</mi>
	</mrow>
	<mo>.</mo>
</math>
</td>
<td>(2.1.3)</td>
</tr>
</table>
</p>
<p>В русском алфавите с учетом пробела 34 знака. Если считать, что в тексте частота появления знаков одинакова, то в опыте с угадыванием каждого знака алфавита содержится следующая энтропия:
</p>
<math display="block">
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo><mo>(</mo><mi>n</mi><mo>)</mo>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mn>34</mn>
	</mrow>	
<mo>&#8776;</mo>
	<mrow>
		<mn>5.087</mn><mtext>&nbsp; бит.</mtext>
	</mrow>
</math>

<p>На количество информации, получаемой из сообщения, влияет фактор неожиданности его для получателя, который
зависит от вероятности получения того или иного сообщения. Чем меньше эта вероятность, тем сообщение более неожиданно и, следовательно, более информативно. Сообщение, вероятность которого высока и, соответственно, низка степень неожиданности, несет немного информации. Р. Хартли понимал, что сообщения имеют различную вероятность и, следовательно, неожиданность их появления для получателя неодинакова. Но, определяя количество информации, он пытался полностью исключить фактор &quot;неожиданности&quot;. Поэтому формула Хартли позволяет определить количество информации только для случая, когда случайное событие с равновероятными и статистически независимыми исходами. На практике эти условия редко выполняются. При определении количества информации необходимо учитывать не только количество разнообразных сообщений, которые можно получить от источника, но и вероятность их получения.
</p>
<p>Рассмотрим некоторые примеры вычисления количества информации <var>I</var>, как наименьшего количества вспомогательных опытов, необходимых для получения правильного ответа на интересующий нас вопрос. Как правило, вспомогательные опыты ставятся так, чтобы они имели равновероятные исходы, что позволяет &quot;снимать&quot; наибольшую неопределенность. Здесь следует иметь в виду тот факт, что энтропия <var>H</var> в исходе случайного события и количество информации <var>I</var> в сообщении об исходе этого события могут находиться в следующих соотношениях:</p>
<ol>
<li>
<math><mi>H</mi><mo>&gt;</mo><mi>I</mi></math>
 &#8212; сообщение только частично раскрывает неопределенность в исходе опыта;</li>
<li>
<math><mi>H</mi><mo>&#8804;</mo><mi>I</mi></math>
 &#8212; сообщение полностью раскрывает неопределенность в исходе опыта.</li>
</ol>
<div class="examp">
<p><span class="primer">Пример 2.1.1</span>. Студенты из группы, в которой учится 25 человек, загадали одного из студентов. Сколько вопросов надо задать, чтобы отгадать выбранного студента, если группа на все вопросы отвечает &quot;да&quot; или &quot;нет&quot;?</p>
<p><span class="primer">Решение</span>. На первом шаге разобъем группу на возможно близкие по численности подгруппы: первая подгруппа пусть из 13 студентов, а вторая &#8212; из оставшихся 12 студентов. В предположении равновероятности принадлежности студента любой из подгрупп спросим, относится ли студент к одной из них, что позволит отбросить одну из подгрупп. Ответ будет содержать 1 бит информации.
</p>
<p>На втором шаге оставшуюся подгруппу (например, из 13 студентов) опять разобъем на две части из 7 и 6 студентов соответственно. Ответ на вопрос о принадлежности студента одной из частей позволит выбрать одну из них.</p>
<p>Пусть выбранная часть группы состоит из 7 студентов. Допустим, что на третьем шаге вновь выбранная подгруппа состоит из 4 студентов. На четвертом шаге сократим количество студентов в подгруппе до двух человек. На пятом шаге из двух студентов выберем нужного.</p>
<p>Таким образом, всего потребуется не более 5 вопросов, а ответы на них содержат 5 бит информации.</p>
<p>При вычислении достаточного количества вопросов для отгадывания нужного студента нам пришлось делить группу на две подгруппы с неравным количеством студентов. Тогда события &quot;нужный студент принадлежит 1-ой подгруппе&quot; и &quot;нужный студент принадлежит 2-ой подгруппе&quot; неравновероятны. Поэтому во вспомогательных опытах мы нарушаем условие равновероятности исходов опыта. Вспомогательные опыты можно было применить к группе из 2<sup>5</sup>=32 студентов, где 32 есть наименьшая степень числа 2, ограничивающая сверху число 25. Тогда на каждом шаге мы будем иметь  подгруппы из одинакового количества студентов, и всего понадобится 5 вопросов.</p>
<p>По формуле Хартли энтропия опыта в отгадывании студента находится как
<math>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo>
		<mrow>
			<mo>(</mo><mn>25</mn><mo>)</mo>
		</mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mn>25</mn>
	</mrow>
</math>
 бит, что больше 4 и меньше 5.</p>
</div>
<div class="examp">
<p><span class="primer">Пример 2.1.2</span>. Сколько вопросов надо задать, чтобы отгадать месяц рождения человека, если на все вопросы он отвечает &quot;да&quot; или &quot;нет&quot;?</p>
<p><span class="primer">Решение</span>. По формуле Хартли энтропия опыта в отгадывании месяца рождения равняется
<math>
<mrow>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo>
		<mrow><mo>(</mo><mn>12</mn><mo>)</mo></mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mn>12</mn>
	</mrow>
</mrow>
</math>
бит. Величина 
<math>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo>
		<mrow><mo>(</mo><mn>12</mn><mo>)</mo></mrow>
	</mrow>
</math>
больше 3, но меньше 4 бит. Очевидно, что трех вопросов может оказаться мало, а четырех вопросов достаточно.</p>
</div>

<div class="examp">
<p><span class="primer">Пример 2.1.3</span>. Имеется 9 монет одного достоинства, одна из которых фальшивая, отличающаяся от остальных по весу (причем неизвестно, легче она или тяжелее настоящих). Каково наименьшее число взвешиваний на чашечных весах без гирь, которое позволяет обнаружить фальшивую монету?</p>
<p><span class="primer">Решение</span>. Любая из 9 монет может оказаться фальшивой, поэтому всего 18 равновероятных исходов. Энтропию опыта <var>b</var> по нахождению фальшивой монеты вычислим по формуле Хартли:
<math>
<mrow>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo>
			<mrow><mo>(</mo><mi>b</mi><mo>)</mo></mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mn>18</mn>
	</mrow>
</mrow>
</math>
 бит. Тогда сообщение <var>a</var> о фальшивой монете должно содержать не менее
<math>
<mrow>
	<msub><mi>log</mi><mi>2</mi></msub>
	<mo>&#x2061;</mo>
	<mn>18</mn>
</mrow>
</math>
 бит информации: 
<math>
<mrow>
	<mrow>
		<mi>I</mi><mo>&#x2061;</mo>
		<mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow>
	</mrow>		
		<mo>&#8805;</mo>
	<mrow>	
		<mi>H</mi><mo>&#x2061;</mo>
		<mrow><mo>(</mo><mi>b</mi><mo>)</mo></mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mi>2</mi></msub>
		<mo>&#x2061;</mo>
		<mn>18</mn>
	</mrow>
</mrow>
</math>.</p>
<p>Взвешивание может иметь три равновероятных исхода, поэтому неопределенность этого опыта составляет по формуле Хартли
<math>
<mrow>
	<msub><mi>log</mi><mn>2</mn></msub>
	<mo>&#x2061;</mo>
	<mn>3</mn>
</mrow>
</math>
 бит. Пусть потребуется <var>k</var> взвешиваний: 
<math>
<mrow>
	<mi>a</mi>
	<mo>=</mo>
	<mrow>
		<msub><mi>a</mi><mn>1</mn></msub>
		<mo>&#x2063;</mo>
		<msub><mi>a</mi><mn>2</mn></msub>
		<mo>&#x2063;</mo><mi>...</mi>
		<mo>&#x2063;</mo>
		<msub><mi>a</mi><mn>k</mn></msub>
	</mrow> 
</mrow>
</math>.
 Тогда всего информации об исходах взвешиваний будет 
<math>
	<mrow>
		<mi>I</mi>
		<mo>&#x2061;</mo>
		<mrow>
			<mo>(</mo><mi>a</mi><mo>)</mo>
		</mrow>
	</mrow>
	<mo>=</mo>
	<mrow>
		<mi>k</mi><mo>&#x2062;</mo>
		<mrow>
			<msub><mi>log</mi><mn>2</mn></msub>
			<mo>&#x2061;</mo>
			<mn>3</mn>
		</mrow>
	</mrow>
</math>
 бит. Количество информации в сообщении об исходе вспомогательного опыта должно быть не меньше энтропии опыта по обнаружению фальшивой монеты:
<math>
<mrow>
	<mrow><mi>k</mi>
		<mo>&#x2062;</mo>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mn>3</mn>
	</mrow>
	<mo>&#8805;</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub>
		<mo>&#x2061;</mo>
		<mn>18</mn>
	</mrow>
</mrow>
</math>
 или
<math>
 <mrow>
	<mi>k</mi><mo>&#8805;</mo><mn>3</mn>
 </mrow>
</math>
 .</p>
<p>Процедуру взвешивания можно построить так:</p> 
<ol>
<li>Разобъем 9 монет на три части по 3 монеты в каждой. За два взвешивания определим ту часть, которая отлична по весу от остальных и, следовательно, содержит фальшивую монету. Если она тяжелее остальных двух частей, то фальшивая монета тяжелее настоящей, в противном случае &#8212; легче.
</li>
<li>Мы имеем 3 монеты, одна из которых фальшивая. Знаем и отличие во весу (легче или тяжелее) фальшивой монеты от настоящей. Очевидно, что в этих условиях достаточно одного взвешивания для выделения из трех монет фальшивой.
</li>
<p>Таким образом, для нахождения фальшивой монеты потребуется 3 взвешивания.</p>
</ol>
</div>
<h3>Контрольные вопросы и упражнения</h3>
<ol>
<li>Почему в определении энтропии как меры неопределенности выбрана логарифмическая зависимость
<math>
<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow>
<mo>=</mo>
<mrow>
	<msub><mi>log</mi><mn>2</mn></msub><mo>&#x2061;</mo>
	<mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow>
</mrow>
</math>
?</li>
<li>Какова энтропия следующих опытов:</li>
<ol type="a"><li>бросок монеты;</li>
<li>бросок игральной кости;</li>
<li>вытаскивание наугад одной игральной карты из 36;</li>
<li>бросок двух игральных костей?</li>
</ol>
<li>Алфавит русского языка содержит 34 буквы (с пробелом), английского &#8212; 27. Если считать частоту появления всех букв в тексте величиной постоянной, то как соотносятся неопределенности, связанные с угадыванием случайно выбранной буквы текста?
</li>
<li>Какое количество информации связано с исходом следующих опытов:
<ol type="a">
<li>бросок игральной кости;</li>
<li>бросок двух монет;</li>
<li>вытаскивание наугад одной игральной карты из 36;</li>
<li>бросок двух игральных костей?</li>
</ol>
</li>
<li>Мы отгадываем задуманное кем-то двузначное число. Какое количество информации требуется для отгадывания всего числа? Какова оптимальная последовательность вопросов при отгадывании? Каково их минимальное число? Изменится ли требуемое количество информации, если мы будем отгадывать не все число сразу, а по очереди: сначала первую цифру, а затем вторую? Одинакова ли информация, необходимая для отгадывания первой и второй цифр?</li>
</ol>
<p><a href="1_2.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="2_2.html">К следующему</a></p>
</body>
<script type="text/javascript">
  MathJax.Hub.Configured()
</script>
</html>