<!DOCTYPE html>
<html>
<meta charset="UTF-8" />
<link rel='stylesheet' type="text/css" href="style.css"></link>
<meta name="author" content="dima" />
<meta name="description" content="Кодирование символьной информации" />
<meta name="keywords" content="постановка задачи кодирования, первая теорема Шеннона, кодирование, декодирование, избыточность кода, первичное сообщение, вторичное сообщение" />
<head>
	<script type="text/x-mathjax-config">MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML"],
  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<title>Кодирование символьной информации</title>
</head>
<body>
<p><a href="2_5.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="3_2.html">К следующему</a></p>
<h1>3. Кодирование символьной информации</h1>
<p>В данном разделе познакомимся с принципами наиболее экономичного кодирования информации, которые касаются не только передачи, но и обработки и хранения информации.</p>
<h2>3.1 Постановка задачи кодирования. Первая теорема Шеннона</h2>
<p>Как отмечалось при рассмотрении исходных понятий информатики, для представления дискретного сообщения используется некоторый <a href="1_2.html#alphabit">алфавит</a>. Очевидно, что одна и та же информация может быть представлена посредством различных алфавитов. В связи с такой возможностью возникает проблема перехода от одного алфавита к другому без потери информации. Условимся называть алфавит, с помощью которого представляется информация до преобразования, <span class="termin">первичным</span>, алфавит конечного представления &#8212; <span class="termin">вторичным</span>.</p>
<p>Введем ряд определений:</p>
<p><span class="teorema">Код &#8212; это правило, описывающее соответствие знаков или их сочетаний одного алфавита знакам или их сочетаниям другого алфавита.</span></p>
<p><span class="teorema">Код (или кодовое слово) &#8212; это последовательность знаков вторичного алфавита, используемых для представления знаков или их сочетаний первичного алфавита.</span></p>
<p><span class="teorema">Кодирование &#8212; перевод информации, представленной посредством первичного алфавита, в последовательность кодов, а декодирование &#8212; операция, обратная кодированию, т.е. восстановление информации в первичном алфавите по полученной последовательности кодов.</span></p>
<p><span class="teorema">Операции кодирования и декодирования называются обратимыми, если их последовательное применение обеспечивает возврат к исходной информации без каких-либо ее потерь.</span></p>
<p>Примером обратимого кодирования является представление знаков в телеграфном коде и их восстановление после передачи. Примером необратимого кодирования может служить перевод с одного естественного языка на другой &#8212; обратный перевод, вообще говоря, не восстанавливает исходный текст. В дальнейшем изложении ограничим себя рассмотрением только обратимого кодирования.</p>
<p>Таким образом, кодирование предшествует передаче и хранению информации. При этом, как указывалось ранее, хранение связано с фиксацией некоторого состояния носителя информации, а передача &#8212; с изменением состояния с течением времени (т.е. процессом). Эти состояния или изменения состояний будем называть элементарными сигналами &#8212; именно их совокупность и составляет вторичный алфавит.</p>
<p>Не обсуждая технических сторон передачи и хранения сообщения, ограничимся математической постановкой задачи кодирования.</p>
<p><a name="srdlina"></a>Пусть первичный алфавит <var>A</var> содержит <var>N</var> знаков с энтропией 
<math display='inline'>
	<mrow>
		<mi>H</mi>
		<mo>&#x2061;</mo>
		<mrow>
			<mo>(</mo><mi>A</mi><mo>)</mo>
		</mrow>
</mrow>
</math>, а вторичный алфавит <var>B</var> &#8212; <var>M</var> знаков с энтропией 
<math display='inline'>
<mrow>
		<mi>H</mi>
		<mo>&#x2061;</mo>
	<mrow>
		<mo>(</mo><mi>B</mi><mo>)</mo>
	</mrow>
</mrow></math>. Пусть также исходное сообщение, представленное в первичном алфавите, содержит <var>n</var> знаков, а закодированное сообщение &#8212; <var>m</var> знаков. Если исходное сообщение содержит 
<math display='inline'>
<mrow>
	<msub><mi>I</mi><mn>1</mn></msub>
</mrow></math>
информации, а закодированное &#8212; <math display='inline'><mrow><msub><mi>I</mi><mn>2</mn></msub></mrow></math>, то условие обратимости кодирования, очевидно, может быть записано как 
<math display='inline'>
<mrow>
	<mrow><msub><mi>I</mi><mn>1</mn></msub></mrow>
	<mo>&lt;</mo>
	<mrow><msub><mi>I</mi><mn>2</mn></msub></mrow>
</mrow>
</math>, смысл которого в том, что операция обратимого кодирования может увеличить количество формальной информации
в сообщении, но не может его уменьшить. Каждая из величин в данном неравенстве может быть заменена произведением числа знаков на энтропию:
<a name="f3.1.1"></a>
<table class='formula'><tr>
<td class='formula'>
<math display="block">
<mrow>
	<mrow>
		<mi>n</mi><mo>&#x2062;</mo>
			<mrow>
				<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
			</mrow>
	</mrow>
	<mo>&lt;</mo>
	<mrow>
		<mi>m</mi><mo>&#x2062;</mo>
			<mrow>
				<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>B</mi><mo>)</mo></mrow>
			</mrow>
	</mrow>
</mrow><mo>.</mo>
</math> 
</td>
<td class='nomer'>(3.1.1)</td>
</tr></table>
</p>
<a name="dl_cod"></a>
<p><span class="teorema">Отношение
<math display='inline'>
<mrow>
  <msup>
    <mi>K</mi>
    <mi>B</mi>
  </msup>
  <mo>=</mo>
  <mfrac>
    <mi>m</mi>
    <mi>n</mi>
  </mfrac>
</mrow>
</math>
, выражающее среднее число знаков вторичного алфавита <var>B</var>, которое приходится использовать для кодирования одного знака первичного алфавита <var>A</var>, назовем средней длиной кода</span>.
</p>
<a name="izb_kod"></a>
<p>По аналогии с величиной <var>R</var>, характеризующей <a href="2_5.html#izb_lang">избыточность языка</a>, можно ввести <span class="termin">относительную избыточность кода</span> <var>Q</var>:
<a name="f3.1.2"></a>
<table class='formula'><tr>
<td class='formula'>
<math display="block">
<mrow>
  <mi>Q</mi>
  <mo>=</mo>
  <mrow>
	  <mn>1</mn><mo>−</mo><mfrac><msub><mi>I</mi><mi>A</mi></msub><msub><mi>I</mi><mi>B</mi></msub></mfrac>
  </mrow>
  <mo>=</mo>
  <mrow>
	  <mn>1</mn><mo>−</mo>
	  	<mfrac>
	  		<mrow>
	  			<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
	  		</mrow>
    		<mrow>
    			<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>B</mi><mo>)</mo></mrow>
				<mo>&#x2062;</mo>		      
		      <msup><mi>K</mi><mi>B</mi></msup>
    		</mrow>
  		</mfrac>
  </mrow>
</mrow><mo>.</mo>
</math>
</td>
<td class='nomer'>(3.1.2)</td>
</tr></table></p>
<p>Данная величина показывает, насколько операция кодирования увеличила длину исходного сообщения. Очевидно, чем ближе <var>Q</var> к 0, тем более выгодным оказывается код и более эффективной операция кодирования, поскольку на передачу более короткого кода затрачивается меньше энергии и времени, а на хранение требуется меньше памяти. При этом следует сознавать, что выгодность кода не идентична временной выгодности всей цепочки &quot;кодирование &#8212; передача &#8212; декодирование&quot;, так как возможна ситуация, когда за использование эффективного кода при передаче придется расплачиваться достаточно трудоемкими операциями кодирования и декодирования.</p>
<p>Выражение <a href="3_1.html#f3.1.1">(3.1.1)</a> пока следует воспринимать как соотношение оценочного характера, из
которого неясно, в какой степени при кодировании возможно приближение к равенству его правой и левой частей. По этой причине для теории связи важнейшее значение имеют две теоремы, доказанные Шенноном. Первая &#8212; ее мы сейчас рассмотрим &#8212; затрагивает ситуацию с кодированием при передаче сообщения по линии связи, в которой отсутствуют помехи, искажающие информацию. <a href="5_1.html#teorema2">Вторая теорема Шеннона</a> посвящена вопросу кодирования при передаче сообщения по линиям связи с помехами.</p>
<a name="t_shennon"></a>
<p><span class="termin">Первая теорема Шеннона</span> о передаче информации, которая называется также <span class="termin">основной теоремой о кодировании при отсутствии помех</span>, формулируется следующим образом:</p>
<p><span class="name">Теорема 3.1.1.</span><span class="teorema"> При отсутствии помех передачи всегда возможен такой вариант кодирования сообщения, при котором среднее число знаков кода, приходящихся на один знак кодируемого алфавита, будет сколь угодно близко к отношению средних информаций на знак первичного и вторичного алфавитов.</span></p>
<p>В терминах избыточности кода можно дать более короткую формулировку теоремы:</p>
<p><span class="name">Теорема 3.1.1'.</span><span class="teorema">При отсутствии помех передачи всегда возможен такой вариант кодирования сообщения, при котором избыточность кода будет сколь угодно близкой к нулю.</span></p>
<p>Доказательство первой теоремы Шеннона опустим, адресовав интересующихся доказательной стороной к книге А. М. и И. М. Ягломов <a href="litra.html#42">[43], с. 211-225</a>. Теорема открывает принципиальную возможность оптимального кодирования, но не указывает практические способы такого кодирования. Для этого должны привлекаться какие-то дополнительные соображения, что и станет предметом нашего последующего обсуждения.</p>
<p>В дальнейшем ограничимся ситуацией, когда <var>M=2</var>, т.е. для представления кодов в линии связи используется лишь два типа сигналов &#8212; с технической точки зрения это наиболее просто реализуемый вариант (например, существование напряжения в проводе (будем называть это импульсом) или его отсутствие (пауза); наличие или отсутствие отверстия на перфокарте или намагниченной области на дискете). Подобное кодирование называется двоичным. Знаки двоичного алфавита принято обозначать &quot;<var>0</var>&quot; и &quot;<var>1</var>&quot;.<a name="dv_slovo"> Последовательность из </a> &quot;<var>0</var>&quot; и &quot;<var>1</var>&quot; образует <span class="name">двоичное слово</span> или двоичный код. Удобство двоичных кодов и в том, что при равных длительностях и вероятностях каждый элементарный сигнал (&quot;<var>0</var>&quot; или &quot;<var>1</var>&quot;) несет в себе 1 бит информации: 
<math display='inline'>
<mrow>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>B</mi><mo>)</mo></mrow>	
	</mrow>
	<mo>=</mo>
	<mrow>
		<msub><mi>log</mi><mn>2</mn></msub><mo>&#x2061;</mo><mi>M</mi>
	</mrow>
	<mo>=</mo>
	<mn>1</mn>	
</mrow>
</math>. Тогда из <a href="3_1.html#f3.1.1">(3.1.1)</a> следует <a name="f3.1.3"></a>
<table class='formula'><tr>
<td class='formula'>
<math display="block">
<mrow>
	<mrow>
		<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow>	
	</mrow>
	<mo>&lt;</mo>
	<msup><mi>K</mi><mi>B</mi></msup>
</mrow><mo>,</mo>
</math></td>
<td class='nomer'>(3.1.3)</td>
</tr></table>
а из <a href="3_1.html#f3.1.2">(3.1.2)</a> получаем формулу для избыточности двоичного кода:
<a name="f3.1.4"></a>
<table class='formula'>
<tr>
<td class='formula'>
<math display="block">
<mrow>
  <mi>Q</mi>
  <mo>=</mo>
  <mrow>
	  <mn>1</mn><mo>−</mo>
	  	<mfrac>
	  		<mrow>
	  			<mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
	  		</mrow>
    		<mrow>
		      <msup><mi>K</mi><mi>B</mi></msup>
    		</mrow>
  		</mfrac>
  </mrow>
</mrow><mo>.</mo>
</math></td>
<td class='nomer'>(3.1.4)</td>
</tr></table></p>
<p>Отсюда первая теорема Шеннона получает следующую интерпретацию:</p>
<p><span class="name">Теорема 3.1.1&#8217;&#8217;.</span><span class="teorema">При отсутствии помех передачи средняя длина двоичного кода может быть сколь угодно близкой к энтропии первичного алфавита.</span></p>
<p>Определение количества переданной информации при двоичном кодировании сводится к простому подсчету числа
импульсов (единиц) и пауз (нулей). При этом возникает проблема выделения из потока сигналов (последовательности импульсов и пауз) отдельных кодов. Приемное устройство фиксирует интенсивность и длительность сигналов. Элементарные сигналы &quot;0&quot; и &quot;1&quot; могут иметь одинаковые или разные длительности. Длительность двоичного элементарного импульса показывает, сколько времени требуется для передачи 1 бита информации. Количество элементарных сигналов в коде (длина кодовой цепочки), который ставится в соответствие знаку первичного алфавита, также может быть одинаковым (в этом случае код называется равномерным) или разным (неравномерный код). Наконец, коды могут строиться для каждого знака исходного алфавита (алфавитное кодирование) или для их комбинаций (кодирование блоков, слов). В результате при кодировании (алфавитном и словесном) возможны следующие варианты сочетаний:</p>
<table id="table8" border="1" align="center">
<caption>Таблица 3.1.1.</caption>
    <tr>
      <td> -</td>
      <td>Длительности элементарных сигналов</td>
      <td>Кодировка первичных символов (слов)</td>
    </tr>
    <tr>
      <td>(1)</td><td>одинаковые</td><td>равномерная</td>
    </tr>
    <tr>
      <td>(2)</td><td>одинаковые</td><td>неравномерная</td>
    </tr>
    <tr>
      <td>(3)</td><td>разные</td><td>равномерная</td>
    </tr>
    <tr>
      <td>(4)</td><td>разные</td><td>неравномерная</td>
    </tr>
</table>
<p>В случае использования неравномерного кодирования или сигналов разной длительности (ситуации (2), (3) и (4)) для отделения кода одного знака от другого между ними необходимо передавать специальный сигнал &#8212; временной
разделитель (признак конца знака) или применять такие коды, которые оказываются уникальными, т.е. несовпадающими с частями других кодов. При равномерном кодировании одинаковыми по длительности сигналами (ситуация (1)) передачи специального разделителя не требуется, поскольку отделение одного кода от другого производится по общей длительности, которая для всех кодов оказывается одинаковой (или одинаковому числу бит при хранении).</p>
<p>Очевидно, длительность передачи информации, в среднем приходящейся на знак первичного алфавита, пропорциональна средней длине двоичного кода <var>K<sup>B</sup></var>. Следовательно, <span class="termin">задачу оптимизации кодирования (или построения экономичного кода)</span> можно сформулировать так:</p>
<a name="optim"></a>
<p><span class="teorema">Построить такую систему кодирования, чтобы суммарная длительность передачи кодов (или суммарное число кодов при хранении) данного сообщения была бы наименьшей.</span></p>
<p>В следующих параграфах рассмотрим различные способы построения кодов.</p>
<h3>Контрольные вопросы и упражнения</h3>
<ol>
<li>Что такое кодирование информации в общем смысле?</li>
<li>Каково место кодирования среди процессов обработки информации?</li>
<li>Что такое код? Приведите примеры кодирования и декодирования.</li>
<li>Приведите пример алфавита кодирования.</li>
<li>Какие коды называются двоичными? Приведите примеры.</li>
<li>Приведите примеры обратимого и необратимого кодирования сообщений.</li>
<li>Может ли обратимое кодирование уменьшить количество формальной информации в сообщении?</li>
<li>Дайте определение средней длины кода.</li>
<li>В каких единицах измеряется средняя длина кода 
<math display='inline'>
	<msup><mi>K</mi><mi>B</mi></msup>
</math>?</li>
<li>В каких единицах измеряется средняя длина двоичного кода 
<math display='inline'>
	<msup><mi>K</mi><mi>B</mi></msup>
</math>?</li>
<li>В чем смысл первой теоремы Шеннона?</li>
<li>Дайте определение относительной избыточности кода.</li>
<li>В каких единицах измеряется избыточность кода?</li>
<li>Информационная емкость исходного сообщения составляет 45 бит, а закодирванного - 76 бит. Вычислите относительную избыточность кода <var>Q</var>.</li>
<li>Средняя длина кода
<math display='inline'>
<mrow>
	<msup><mi>K</mi><mi>B</mi></msup><mo>=</mo><mn>5.7</mn>
</mrow>
</math>
знака вторичного алфавита, энтропия исходного сообщения 
<math display='inline'>
<mrow>
	<mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mrow>
	<mo>=</mo><mn>4.34</mn>
</mrow>
</math>
бита, а энтропия закодированного сообщения &#8212; 
<math display='inline'>
<mrow>
	<mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mrow>
	<mo>=</mo><mn>5.74</mn>
</mrow>
</math>
бита. Вычислите относительную избыточность кода.</li>
<li>Энтропия первичного сообщения составляет 
<math display='inline'>
<mrow>
	<mrow><mi>H</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mrow>
	<mo>=</mo><mn>4.34</mn>
</mrow>
</math>
бита, средняя длина двоичного кода первичного сообщения 
<math display='inline'>
	<mrow>
		<msup><mi>K</mi><mi>B</mi></msup>
		<mo>=</mo>
		<mn>5.7</mn>
	</mrow>
</math>
бита. Вычислите относительную избыточность двоичного кода.</li>
</ol>
<p><a href="2_5.html">К предыдущему</a> <a href="index.html">К содержанию</a> <a href="3_2.html">К следующему</a></p>
</body>
<script type="text/javascript">
  MathJax.Hub.Configured()
</script>
</html>